{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "import pathlib\n",
    "import pydash\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import unidecode\n",
    "\n",
    "def value_extract(row, col):\n",
    "\n",
    "    ''' Extract dictionary values. '''\n",
    "  \n",
    "    return pydash.get(row[col], \"value\")\n",
    "   \n",
    "def sparql_query(query, service):\n",
    " \n",
    "    ''' Send sparql request, and formulate results into a dataframe. '''\n",
    "\n",
    "    r = requests.get(service, params={\"format\": \"json\", \"query\": query})\n",
    "    data = pydash.get(r.json(), \"results.bindings\")\n",
    "    data = pandas.DataFrame.from_dict(data)\n",
    "    for x in data.columns:\n",
    "        data[x] = data.apply(value_extract, col=x, axis=1)\n",
    " \n",
    "    return data\n",
    "\n",
    "def normalise(row, col):\n",
    "\n",
    "    ''' Normalise text for matching purposes. '''\n",
    "\n",
    "    norm = unidecode.unidecode(str(row[col]).lower()).strip()\n",
    "\n",
    "    return norm\n",
    "\n",
    "def median_score(a_list, b_id, f):\n",
    "\n",
    "    ''' Find best match per against lists, return median. '''\n",
    "\n",
    "    test = wikidata.loc[wikidata.director_wikidata.isin([b_id])]\n",
    "    b_list = test.film_label.unique()\n",
    "    if len(a_list) < f or len(b_list) < f:\n",
    "        return 0\n",
    "\n",
    "    my_score = [process.extractOne(a, b_list, scorer=fuzz.WRatio)[1] for a in a_list]\n",
    "    return numpy.median(my_score)\n",
    "\n",
    "data_path = pathlib.Path.cwd() / 'sight_and_sound.parquet'\n",
    "\n",
    "if not data_path.exists():\n",
    "\n",
    "    index = requests.get('https://www.bfi.org.uk/sight-and-sound/greatest-films-all-time/all-voters').text\n",
    "    index = index.split('<script type=\"text/javascript\">var initialPageState = ')[1].split('</script>')[0]\n",
    "    dataframe = pandas.DataFrame(pydash.get(json.loads(index), 'componentState.allVoters'))\n",
    "    dataframe = dataframe[['firstname', 'surname', 'type', 'country', 'url']]\n",
    "\n",
    "    votes = pandas.DataFrame()\n",
    "    for x in tqdm.tqdm(dataframe.url.unique()):\n",
    "\n",
    "        time.sleep(4)\n",
    "\n",
    "        vote_page = pandas.read_html('https://www.bfi.org.uk'+x, encoding='utf8')[0]\n",
    "        vote_page['url'] = x\n",
    "        if len(vote_page) != 10:\n",
    "            print(pathlib.Path(x).stem, 'should be only ten votes')\n",
    "        votes = pandas.concat([votes, vote_page])\n",
    "\n",
    "    dataframe = pandas.merge(dataframe, votes, on='url', how='left')\n",
    "    dataframe = dataframe.astype(str)\n",
    "    dataframe.to_parquet(data_path)\n",
    "else:\n",
    "    dataframe = pandas.read_parquet(data_path)\n",
    "\n",
    "dataframe['Film'] = dataframe.apply(normalise, col='Film', axis=1)\n",
    "dataframe['Director'] = dataframe.apply(normalise, col='Director', axis=1)\n",
    "\n",
    "print(len(dataframe))\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_path = pathlib.Path.cwd() / 'wikidata.parquet'\n",
    "\n",
    "if not wikidata_path.exists():\n",
    "    wikidata = pandas.DataFrame()\n",
    "    for year in tqdm.tqdm(range(1880, 2025)):\n",
    "        query = '''select ?film ?filmLabel ?title ?director ?directorLabel (year(?publication_date) as ?year) \n",
    "            where {\n",
    "                ?film p:P31/wdt:P279* ?state .\n",
    "                ?state ps:P31/wdt:P279* wd:Q11424 .\n",
    "                ?film  wdt:P577 ?publication_date .\n",
    "                filter (year(?publication_date) = '''+str(year)+''') .\n",
    "                ?film wdt:P57 ?director\n",
    "                optional { ?film wdt:P1476 ?title } .\n",
    "                service wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}'''\n",
    "        extract = sparql_query(query, \"https://query.wikidata.org/sparql\")\n",
    "        wikidata = pandas.concat([wikidata, extract])\n",
    "\n",
    "    for x in ['film', 'director']:\n",
    "        wikidata[x] = wikidata[x].str.split('/').str[-1]\n",
    "\n",
    "    wikidata = pandas.concat([\n",
    "        wikidata[[x for x in wikidata.columns.values if x != 'filmLabel']],\n",
    "        wikidata[[x for x in wikidata.columns.values if x != 'title']].rename(columns={'filmLabel':'title'})\n",
    "        ]).dropna().drop_duplicates()\n",
    "\n",
    "    wikidata = wikidata.rename(columns={\n",
    "        'film':'film_wikidata', 'director':'director_wikidata', 'title':'film_label', 'directorLabel':'director_label'})\n",
    "    \n",
    "    wikidata = wikidata.astype(str)\n",
    "    wikidata.to_parquet(wikidata_path)\n",
    "else:\n",
    "    wikidata = pandas.read_parquet(wikidata_path)\n",
    "\n",
    "wikidata['film_label'] = wikidata.apply(normalise, col='film_label', axis=1)\n",
    "wikidata['director_label'] = wikidata.apply(normalise, col='director_label', axis=1)\n",
    "\n",
    "print(len(wikidata)) \n",
    "wikidata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_path = pathlib.Path.cwd() / 'match.parquet'\n",
    "\n",
    "if not match_path.exists():\n",
    "\n",
    "    name_match_score = 60 # name matching tolerance\n",
    "    title_match_score = 100 # title matching tolerance\n",
    "    minimum_match_candidates = 4 # minimum matching options.\n",
    "\n",
    "    result_dataframe = pandas.DataFrame(columns=['Director', 'director_wikidata'])\n",
    "    for x in tqdm.tqdm(dataframe.Director.unique()[:4]):\n",
    "        focus = dataframe.loc[dataframe.Director.isin([x])]\n",
    "        c = process.extract(x, wikidata.director_label.unique(), scorer=fuzz.WRatio, limit=200)\n",
    "        c = [y[0] for y in c if y[1] > name_match_score]\n",
    "        candidates = wikidata.loc[wikidata.director_label.isin(c)] \n",
    "        result = {y:median_score(focus.Film.unique(), y, minimum_match_candidates) for y in candidates.director_wikidata.unique()}\n",
    "        result = [k for k,v in result.items() if v == title_match_score]\n",
    "        if len(result) == 1:\n",
    "            result_dataframe.loc[len(result_dataframe)] = [(x), (result[0])]\n",
    " \n",
    "    # result_dataframe = result_dataframe.astype(str)\n",
    "    # result_dataframe.to_parquet(match_path)\n",
    "\n",
    "print(len(result_dataframe))\n",
    "result_dataframe.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
