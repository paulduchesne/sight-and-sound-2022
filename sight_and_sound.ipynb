{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"firstname\": \"Pedro Adrián\",\n",
      "    \"surname\": \"Zuluaga\",\n",
      "    \"type\": \"critic\",\n",
      "    \"country\": [\n",
      "        {\n",
      "            \"country\": \"Colombia\",\n",
      "            \"country_id\": \"ce20dbd2-4c27-4728-a7d0-2b8a0c2ce0a4\"\n",
      "        }\n",
      "    ],\n",
      "    \"url\": \"/sight-and-sound/greatest-films-all-time/all-voters/pedro-adrian-zuluaga\",\n",
      "    \"voter_id\": \"86a6ed28-cca8-4d66-90dc-0b82669ee5d3\",\n",
      "    \"votes\": [\n",
      "        {\n",
      "            \"Film\": \"The Virgin Spring\",\n",
      "            \"Year\": 1960,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Ingmar Bergman\",\n",
      "                    \"director_id\": \"0c3b11e7-42e2-4f5b-b333-53381e2ac0ee\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"f51b2d87-c62d-4801-be73-4f1f0d80b33d\"\n",
      "        },\n",
      "        {\n",
      "            \"Film\": \"THÉRÈSE\",\n",
      "            \"Year\": 1986,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Alain Cavalier\",\n",
      "                    \"director_id\": \"369e805c-9f61-4b91-92ac-31c8a79e52f9\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"94bba81b-8636-4c7a-9e1e-dfaf373c4051\"\n",
      "        },\n",
      "        {\n",
      "            \"Film\": \"Limite\",\n",
      "            \"Year\": 1931,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Mário Peixoto\",\n",
      "                    \"director_id\": \"c964188b-a65b-4d9c-a93c-66962a3971a2\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"78d8b8ca-b78d-4cce-8e46-9e1b68b8bc4c\"\n",
      "        },\n",
      "        {\n",
      "            \"Film\": \"La ciénaga\",\n",
      "            \"Year\": 2001,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Lucrecia Martel\",\n",
      "                    \"director_id\": \"a0f7152d-1539-402d-9b0c-2865fe8b5a90\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"6f6665a3-373d-4cda-964f-c236fdc0859a\"\n",
      "        },\n",
      "        {\n",
      "            \"Film\": \"Los olvidados\",\n",
      "            \"Year\": 1950,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Luis Buñuel\",\n",
      "                    \"director_id\": \"1426a26f-b621-4af6-aa2d-45db304c1f2f\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"6a42df0c-be16-4d53-830b-79c9ca91746a\"\n",
      "        },\n",
      "        {\n",
      "            \"Film\": \"Paris, Texas\",\n",
      "            \"Year\": 1984,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Wim Wenders\",\n",
      "                    \"director_id\": \"6509456b-2e7c-4c97-a448-0d681364cee2\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"edea9b3b-f426-4d2c-854a-85cec6c55fd6\"\n",
      "        },\n",
      "        {\n",
      "            \"Film\": \"Tokyo Story\",\n",
      "            \"Year\": 1953,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Yasujirō Ozu\",\n",
      "                    \"director_id\": \"14343da0-b98f-4767-b5d5-e636f421fc67\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"0f1a2c6a-8b03-4ba4-b047-bcc31b9c32b4\"\n",
      "        },\n",
      "        {\n",
      "            \"Film\": \"The Night of the Hunter\",\n",
      "            \"Year\": 1955,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Charles Laughton\",\n",
      "                    \"director_id\": \"6f40fdde-69a8-4466-affe-222f21e8376e\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"f1ee85dc-32a5-412f-8d12-1d1107cda5b2\"\n",
      "        },\n",
      "        {\n",
      "            \"Film\": \"Mouchette\",\n",
      "            \"Year\": 1966,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Robert Bresson\",\n",
      "                    \"director_id\": \"1f0b423d-2fa1-435c-83c4-96c73277548e\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"321f1d4a-ef4a-4cdf-9842-66d5d454bfe7\"\n",
      "        },\n",
      "        {\n",
      "            \"Film\": \"Rosetta\",\n",
      "            \"Year\": 1999,\n",
      "            \"Director\": [\n",
      "                {\n",
      "                    \"director\": \"Luc Dardenne\",\n",
      "                    \"director_id\": \"15e29aa3-9fe6-41ba-9868-126966f36b53\"\n",
      "                },\n",
      "                {\n",
      "                    \"director\": \" Jean-Pierre Dardenne\",\n",
      "                    \"director_id\": \"7ef389aa-8a69-4fbf-ae95-21b69cf5cde8\"\n",
      "                }\n",
      "            ],\n",
      "            \"film_id\": \"326d552c-a9f4-4817-b470-32015bb3b33b\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# from rapidfuzz import process, fuzz\n",
    "# import altair\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "import pathlib\n",
    "import pydash\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "# import unidecode\n",
    "import uuid\n",
    "\n",
    "def value_extract(row, col):\n",
    "\n",
    "    ''' Extract dictionary values. '''\n",
    "  \n",
    "    return pydash.get(row[col], \"value\")\n",
    "   \n",
    "def sparql_query(query, service):\n",
    " \n",
    "    ''' Send sparql request, and formulate results into a dataframe. '''\n",
    "\n",
    "    r = requests.get(service, params={\"format\": \"json\", \"query\": query})\n",
    "    data = pydash.get(r.json(), \"results.bindings\")\n",
    "    data = pandas.DataFrame.from_dict(data)\n",
    "    for x in data.columns:\n",
    "        data[x] = data.apply(value_extract, col=x, axis=1)\n",
    " \n",
    "    return data\n",
    "\n",
    "# def normalise(row, col):\n",
    "\n",
    "#     ''' Normalise text for matching purposes. '''\n",
    "\n",
    "#     norm = unidecode.unidecode(str(row[col]).lower()).strip()\n",
    "\n",
    "#     return norm\n",
    "\n",
    "# def median_score(a_list, b_id, f):\n",
    "\n",
    "#     ''' Find best match per against lists, return median. '''\n",
    "\n",
    "#     test = wikidata.loc[wikidata.director_wikidata.isin([b_id])]\n",
    "#     b_list = test.film_label.unique()\n",
    "#     if len(a_list) < f or len(b_list) < f:\n",
    "#         return 0\n",
    "\n",
    "#     my_score = [process.extractOne(a, b_list, scorer=fuzz.WRatio)[1] for a in a_list]\n",
    "#     return numpy.median(my_score)\n",
    "\n",
    "#\n",
    "# reengineer this so that uuids are only created if they do not exist, and strings are assumed to be unique.\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "def assign_uuid(entity_type, string):\n",
    "    if string not in entities[entity_type].keys():\n",
    "        entities[entity_type][string] = str(uuid.uuid4())\n",
    "\n",
    "    return {entity_type: string, f'{entity_type}_id':entities[entity_type][string]}\n",
    "        # print('FOUND ONE')\n",
    "    # else:\n",
    "    #     entities[entity_type][string] = \n",
    "    #     # return {}\n",
    "    #     # print('NOPOOOOOO')\n",
    "    \n",
    "    # return string\n",
    "\n",
    "    # what you do here, does it already exist in the class, if so return that, otherwise make a new\n",
    "\n",
    "\n",
    "entities = {'country':{}, 'director':{}}\n",
    "\n",
    "\n",
    "data_path = pathlib.Path.cwd() / 'sight_and_sound.json'\n",
    "\n",
    "if not data_path.exists():\n",
    "\n",
    "    index = requests.get('https://www.bfi.org.uk/sight-and-sound/greatest-films-all-time/all-voters').text\n",
    "    index = index.split('<script type=\"text/javascript\">var initialPageState = ')[1].split('</script>')[0]\n",
    "    index = pydash.get(json.loads(index), 'componentState.allVoters')\n",
    "\n",
    "    data = list()\n",
    "\n",
    "    for x in tqdm.tqdm(index):\n",
    "\n",
    "        time.sleep(4)\n",
    "\n",
    "        voter = {k:v for k,v in x.items() if k in ['firstname', 'surname', 'type', 'country', 'url']}\n",
    "        voter['voter_id'] = str(uuid.uuid4())\n",
    "        voter['country'] = [assign_uuid('country', x) for x in voter['country'].split('/')]\n",
    "\n",
    "        votes = pandas.read_html('https://www.bfi.org.uk'+voter['url'], encoding='utf8')[0].to_dict('records')\n",
    "        for y in votes:\n",
    "            y['film_id'] = str(uuid.uuid4())\n",
    "            y['Director'] = [assign_uuid('director', x) for x in str(y['Director']).split(',')]\n",
    "\n",
    "        voter['votes'] = votes\n",
    "        data.append(voter)\n",
    "\n",
    "    with open(data_path, 'w') as write_data:\n",
    "        json.dump(data, write_data, ensure_ascii=False, indent=4)\n",
    "else:\n",
    "    with open(data_path) as read_data:\n",
    "        data = json.load(read_data)\n",
    "\n",
    "print(json.dumps(data[-1], indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconcile countries with wikidata.\n",
    "\n",
    "query = '''select ?country ?countryLabel \n",
    "    where {\n",
    "      values ?status {wd:Q3624078 wd:Q6256 wd:Q779415}\n",
    "        ?country wdt:P31 ?status .\n",
    "        service wikibase:label { bd:serviceParam wikibase:language \"en\". }}'''\n",
    "\n",
    "wikidata_country = sparql_query(query, \"https://query.wikidata.org/sparql\")\n",
    "wikidata_country = wikidata_country.rename(columns={'country':'country_wikidata', 'countryLabel':'country'})\n",
    "wikidata_country['country_wikidata'] = wikidata_country['country_wikidata'].str.split('/').str[-1]\n",
    "\n",
    "country = pandas.json_normalize(data, record_path=['country'])\n",
    "country = country.replace({'country':{\n",
    "    'UK':'United Kingdom', \n",
    "    'Uk':'United Kingdom', \n",
    "    'England':'United Kingdom', \n",
    "    'USA':'United States of America', \n",
    "    'US':'United States of America', \n",
    "    'United States':'United States of America', \n",
    "    'Ireland':'Republic of Ireland',\n",
    "    'China':\"People's Republic of China\",\n",
    "    'Finnland':'Finland',\n",
    "    'france':'France',\n",
    "    'Canda':'Canada',\n",
    "    'Palestine':'State of Palestine', \n",
    "    'Macedonia':'North Macedonia',\n",
    "    'Abu Dhabi':'United Arab Emirates'\n",
    "    }})\n",
    "\n",
    "country = pandas.merge(country, wikidata_country, on='country', how='left').drop_duplicates()\n",
    "country.loc[country.country_wikidata.isin([numpy.nan]), 'country_wikidata'] = None\n",
    "country = country[['country_id', 'country_wikidata']]\n",
    "country = {a:b for a,b in zip(country.country_id, country.country_wikidata)}\n",
    "\n",
    "for x in data:\n",
    "    for y in x['country']:\n",
    "        y['country_wikidata'] = country[y['country_id']]\n",
    "\n",
    "print(json.dumps(data[-1], indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_path = pathlib.Path.cwd() / 'wikidata.parquet'\n",
    "\n",
    "if not wikidata_path.exists():\n",
    "    wikidata = pandas.DataFrame()\n",
    "    for year in tqdm.tqdm(range(1880, 2025)):\n",
    "        query = '''select ?film ?filmLabel ?title ?director ?directorLabel (year(?publication_date) as ?year) \n",
    "            where {\n",
    "                ?film p:P31/wdt:P279* ?state .\n",
    "                ?state ps:P31/wdt:P279* wd:Q11424 .\n",
    "                ?film  wdt:P577 ?publication_date .\n",
    "                filter (year(?publication_date) = '''+str(year)+''') .\n",
    "                ?film wdt:P57 ?director\n",
    "                optional { ?film wdt:P1476 ?title } .\n",
    "                service wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}'''\n",
    "        extract = sparql_query(query, \"https://query.wikidata.org/sparql\")\n",
    "        wikidata = pandas.concat([wikidata, extract])\n",
    "\n",
    "    for x in ['film', 'director']:\n",
    "        wikidata[x] = wikidata[x].str.split('/').str[-1]\n",
    "\n",
    "    wikidata = pandas.concat([\n",
    "        wikidata[[x for x in wikidata.columns.values if x != 'filmLabel']],\n",
    "        wikidata[[x for x in wikidata.columns.values if x != 'title']].rename(columns={'filmLabel':'title'})\n",
    "        ]).dropna().drop_duplicates()\n",
    "\n",
    "    wikidata = wikidata.rename(columns={\n",
    "        'film':'film_wikidata', 'director':'director_wikidata', 'title':'film_label', 'directorLabel':'director_label'})\n",
    "    \n",
    "    wikidata = wikidata.astype(str)\n",
    "    wikidata.to_parquet(wikidata_path)\n",
    "else:\n",
    "    wikidata = pandas.read_parquet(wikidata_path)\n",
    "\n",
    "print(len(wikidata)) \n",
    "wikidata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reconcile filmographies with wikidata.\n",
    "\n",
    "# works = pandas.json_normalize(data, record_path=['votes'])\n",
    "# print(len(works))\n",
    "# works.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe['country'] = dataframe['country'].str.split('/')\n",
    "# dataframe = dataframe.explode('country')\n",
    "# dataframe['country'] = dataframe.apply(normalise, col='country', axis=1)\n",
    "\n",
    "# dataframe = dataframe.replace({'country': {\n",
    "#     'uk':'united kingdom', 'england':'united kingdom', 'usa':'united states of america', \n",
    "#     'united states':'united states of america', 'us':'united states of america', \n",
    "#     'china':\"people's republic of china\", 'ireland':'republic of ireland',\n",
    "#     'canda':'canada', 'palestine':'state of palestine', 'finnland':'finland',\n",
    "#     'macedonia':'north macedonia', 'abu dhabi':'united arab emirates'}})\n",
    "\n",
    "# query = '''select ?country ?countryLabel \n",
    "#     where {\n",
    "#       values ?status {wd:Q3624078 wd:Q6256 wd:Q779415}\n",
    "#         ?country wdt:P31 ?status .\n",
    "#         service wikibase:label { bd:serviceParam wikibase:language \"en\". }}'''\n",
    "\n",
    "# countries = sparql_query(query, \"https://query.wikidata.org/sparql\")\n",
    "# countries = countries.rename(columns={'country':'country_wikidata'})\n",
    "# countries = countries.rename(columns={'countryLabel':'country'})\n",
    "# countries['country'] = countries.apply(normalise, col='country', axis=1)\n",
    "# countries['country_wikidata'] = countries['country_wikidata'].str.split('/').str[-1]\n",
    "\n",
    "# dataframe = pandas.merge(dataframe, countries, on='country', how='left').drop_duplicates()\n",
    "\n",
    "# print(len(dataframe))\n",
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikidata_path = pathlib.Path.cwd() / 'wikidata.parquet'\n",
    "\n",
    "# if not wikidata_path.exists():\n",
    "#     wikidata = pandas.DataFrame()\n",
    "#     for year in tqdm.tqdm(range(1880, 2025)):\n",
    "#         query = '''select ?film ?filmLabel ?title ?director ?directorLabel (year(?publication_date) as ?year) \n",
    "#             where {\n",
    "#                 ?film p:P31/wdt:P279* ?state .\n",
    "#                 ?state ps:P31/wdt:P279* wd:Q11424 .\n",
    "#                 ?film  wdt:P577 ?publication_date .\n",
    "#                 filter (year(?publication_date) = '''+str(year)+''') .\n",
    "#                 ?film wdt:P57 ?director\n",
    "#                 optional { ?film wdt:P1476 ?title } .\n",
    "#                 service wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}'''\n",
    "#         extract = sparql_query(query, \"https://query.wikidata.org/sparql\")\n",
    "#         wikidata = pandas.concat([wikidata, extract])\n",
    "\n",
    "#     for x in ['film', 'director']:\n",
    "#         wikidata[x] = wikidata[x].str.split('/').str[-1]\n",
    "\n",
    "#     wikidata = pandas.concat([\n",
    "#         wikidata[[x for x in wikidata.columns.values if x != 'filmLabel']],\n",
    "#         wikidata[[x for x in wikidata.columns.values if x != 'title']].rename(columns={'filmLabel':'title'})\n",
    "#         ]).dropna().drop_duplicates()\n",
    "\n",
    "#     wikidata = wikidata.rename(columns={\n",
    "#         'film':'film_wikidata', 'director':'director_wikidata', 'title':'film_label', 'directorLabel':'director_label'})\n",
    "    \n",
    "#     wikidata = wikidata.astype(str)\n",
    "#     wikidata.to_parquet(wikidata_path)\n",
    "# else:\n",
    "#     wikidata = pandas.read_parquet(wikidata_path)\n",
    "\n",
    "# wikidata['film_label'] = wikidata.apply(normalise, col='film_label', axis=1)\n",
    "# wikidata['director_label'] = wikidata.apply(normalise, col='director_label', axis=1)\n",
    "\n",
    "# print(len(wikidata)) \n",
    "# wikidata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_path = pathlib.Path.cwd() / 'match.parquet'\n",
    "\n",
    "# if not match_path.exists():\n",
    "\n",
    "#     name_match_score = 60 # name matching tolerance\n",
    "#     title_match_score = 100 # title matching tolerance\n",
    "#     minimum_match_candidates = 4 # minimum matching options.\n",
    "\n",
    "#     result_dataframe = pandas.DataFrame(columns=['Director', 'director_wikidata'])\n",
    "#     for x in tqdm.tqdm(dataframe.Director.unique()):\n",
    "#         focus = dataframe.loc[dataframe.Director.isin([x])]\n",
    "#         c = process.extract(x, wikidata.director_label.unique(), scorer=fuzz.WRatio, limit=200)\n",
    "#         c = [y[0] for y in c if y[1] > name_match_score]\n",
    "#         candidates = wikidata.loc[wikidata.director_label.isin(c)] \n",
    "#         result = {y:median_score(focus.Film.unique(), y, minimum_match_candidates) for y in candidates.director_wikidata.unique()}\n",
    "#         result = [k for k,v in result.items() if v == title_match_score]\n",
    "#         if len(result) == 1:\n",
    "#             result_dataframe.loc[len(result_dataframe)] = [(x), (result[0])]\n",
    " \n",
    "#     result_dataframe = result_dataframe.astype(str)\n",
    "#     result_dataframe.to_parquet(match_path)\n",
    "# else:\n",
    "#     result_dataframe = pandas.read_parquet(match_path)\n",
    "\n",
    "# print(len(result_dataframe))\n",
    "# result_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wd = wikidata.copy()\n",
    "# wd = wd[['director_wikidata', 'film_label', 'film_wikidata']]\n",
    "# wd = wd.rename(columns={'film_label':'Film'}).drop_duplicates()\n",
    "\n",
    "# # result = dataframe.copy()\n",
    "# dataframe = pandas.merge(dataframe, result_dataframe, on='Director', how='left')\n",
    "# dataframe = pandas.merge(dataframe, wd, on=['director_wikidata', 'Film'], how='left')\n",
    "\n",
    "# print(len(dataframe))\n",
    "# dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_dataframe = pandas.DataFrame()\n",
    "# for x in tqdm.tqdm(numpy.array_split(dataframe.dropna().film_wikidata.unique(), 10)):\n",
    "#     time.sleep(2)\n",
    "#     film_values = ' '.join([f'wd:{y}' for y in x])\n",
    "#     query = '''select ?film_wikidata ?wikidata_country \n",
    "#         where {\n",
    "#             values ?film_wikidata {'''+film_values+'''}\n",
    "#             ?film_wikidata wdt:P495 ?wikidata_country .}'''\n",
    "#     country_dataframe = pandas.concat([country_dataframe, sparql_query(query, \"https://query.wikidata.org/sparql\")])\n",
    "\n",
    "# for x in ['film_wikidata', 'wikidata_country']:\n",
    "#     country_dataframe[x] = country_dataframe[x].str.split('/').str[-1]\n",
    "\n",
    "# dataframe = pandas.merge(dataframe, country_dataframe, on='film_wikidata', how='left')\n",
    "# dataframe.loc[dataframe.country_wikidata == dataframe.wikidata_country, 'match'] = 'true'\n",
    "# dataframe.loc[~dataframe.match.isin(['true']), 'match'] = 'false'\n",
    "# dataframe = dataframe.sort_values(by='match', ascending=False)\n",
    "# dataframe = dataframe[['firstname', 'surname', 'film_wikidata', 'country',  'match']].dropna()\n",
    "# dataframe = dataframe.drop_duplicates(subset=['firstname', 'surname', 'film_wikidata', 'country'], keep='first')\n",
    "# dataframe = dataframe.loc[~dataframe.country.isin([''])]\n",
    "# dataframe['country'] = dataframe['country'].str.title()\n",
    "# dataframe['match'] = dataframe['match'].str.title()\n",
    "\n",
    "# print(len(dataframe)) \n",
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altair.data_transformers.enable('default', max_rows=None)\n",
    "# altair.Chart(dataframe).mark_bar().encode(\n",
    "#     x = altair.X('country', title='Country of voter.'),\n",
    "#     y=altair.Y('count(match)', stack=\"normalize\", title='Cinema of own country.'),\n",
    "#     color=altair.Color('match', scale=altair.Scale(domain=['True', 'False'], range=['#653E59', '#C9A6BE']),  \n",
    "#                        sort=['true', 'false'], title='')).properties(width=1500, height=300, title='Sight & Sound 2022 - Patriotic Voters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
