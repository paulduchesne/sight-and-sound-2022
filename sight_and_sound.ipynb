{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2115/2115 [3:47:23<00:00,  6.45s/it]  \n"
     ]
    }
   ],
   "source": [
    "# from rapidfuzz import process, fuzz\n",
    "# import altair\n",
    "import json\n",
    "# import numpy\n",
    "import pandas\n",
    "import pathlib\n",
    "import pydash\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "# import unidecode\n",
    "import uuid\n",
    "\n",
    "# def value_extract(row, col):\n",
    "\n",
    "#     ''' Extract dictionary values. '''\n",
    "  \n",
    "#     return pydash.get(row[col], \"value\")\n",
    "   \n",
    "# def sparql_query(query, service):\n",
    " \n",
    "#     ''' Send sparql request, and formulate results into a dataframe. '''\n",
    "\n",
    "#     r = requests.get(service, params={\"format\": \"json\", \"query\": query})\n",
    "#     data = pydash.get(r.json(), \"results.bindings\")\n",
    "#     data = pandas.DataFrame.from_dict(data)\n",
    "#     for x in data.columns:\n",
    "#         data[x] = data.apply(value_extract, col=x, axis=1)\n",
    " \n",
    "#     return data\n",
    "\n",
    "# def normalise(row, col):\n",
    "\n",
    "#     ''' Normalise text for matching purposes. '''\n",
    "\n",
    "#     norm = unidecode.unidecode(str(row[col]).lower()).strip()\n",
    "\n",
    "#     return norm\n",
    "\n",
    "# def median_score(a_list, b_id, f):\n",
    "\n",
    "#     ''' Find best match per against lists, return median. '''\n",
    "\n",
    "#     test = wikidata.loc[wikidata.director_wikidata.isin([b_id])]\n",
    "#     b_list = test.film_label.unique()\n",
    "#     if len(a_list) < f or len(b_list) < f:\n",
    "#         return 0\n",
    "\n",
    "#     my_score = [process.extractOne(a, b_list, scorer=fuzz.WRatio)[1] for a in a_list]\n",
    "#     return numpy.median(my_score)\n",
    "\n",
    "data_path = pathlib.Path.cwd() / 'sight_and_sound.json'\n",
    "\n",
    "if not data_path.exists():\n",
    "\n",
    "    index = requests.get('https://www.bfi.org.uk/sight-and-sound/greatest-films-all-time/all-voters').text\n",
    "    index = index.split('<script type=\"text/javascript\">var initialPageState = ')[1].split('</script>')[0]\n",
    "    index = pydash.get(json.loads(index), 'componentState.allVoters')\n",
    "\n",
    "    data = list()\n",
    "\n",
    "    for x in tqdm.tqdm(index):\n",
    "\n",
    "        time.sleep(4)\n",
    "\n",
    "        voter = {k:v for k,v in x.items() if k in ['firstname', 'surname', 'type', 'country', 'url']}\n",
    "        voter['voter_id'] = str(uuid.uuid4())\n",
    "        voter['country'] = [{'country':x.strip(), 'country_id':str(uuid.uuid4())} for x in voter['country'].split('/')]\n",
    "\n",
    "        votes = pandas.read_html('https://www.bfi.org.uk'+voter['url'], encoding='utf8')[0].to_dict('records')\n",
    "        for y in votes:\n",
    "            y['film_id'] = str(uuid.uuid4())\n",
    "            y['Director'] = [{'director':x.strip(), 'director_id':str(uuid.uuid4())} for x in str(y['Director']).split(',')]\n",
    "\n",
    "        voter['votes'] = votes\n",
    "        data.append(voter)\n",
    "\n",
    "    with open(data_path, 'w') as write_data:\n",
    "        json.dump(data, write_data, ensure_ascii=False, indent=4)\n",
    "else:\n",
    "    pass # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe['country'] = dataframe['country'].str.split('/')\n",
    "# dataframe = dataframe.explode('country')\n",
    "# dataframe['country'] = dataframe.apply(normalise, col='country', axis=1)\n",
    "\n",
    "# dataframe = dataframe.replace({'country': {\n",
    "#     'uk':'united kingdom', 'england':'united kingdom', 'usa':'united states of america', \n",
    "#     'united states':'united states of america', 'us':'united states of america', \n",
    "#     'china':\"people's republic of china\", 'ireland':'republic of ireland',\n",
    "#     'canda':'canada', 'palestine':'state of palestine', 'finnland':'finland',\n",
    "#     'macedonia':'north macedonia', 'abu dhabi':'united arab emirates'}})\n",
    "\n",
    "# query = '''select ?country ?countryLabel \n",
    "#     where {\n",
    "#       values ?status {wd:Q3624078 wd:Q6256 wd:Q779415}\n",
    "#         ?country wdt:P31 ?status .\n",
    "#         service wikibase:label { bd:serviceParam wikibase:language \"en\". }}'''\n",
    "\n",
    "# countries = sparql_query(query, \"https://query.wikidata.org/sparql\")\n",
    "# countries = countries.rename(columns={'country':'country_wikidata'})\n",
    "# countries = countries.rename(columns={'countryLabel':'country'})\n",
    "# countries['country'] = countries.apply(normalise, col='country', axis=1)\n",
    "# countries['country_wikidata'] = countries['country_wikidata'].str.split('/').str[-1]\n",
    "\n",
    "# dataframe = pandas.merge(dataframe, countries, on='country', how='left').drop_duplicates()\n",
    "\n",
    "# print(len(dataframe))\n",
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikidata_path = pathlib.Path.cwd() / 'wikidata.parquet'\n",
    "\n",
    "# if not wikidata_path.exists():\n",
    "#     wikidata = pandas.DataFrame()\n",
    "#     for year in tqdm.tqdm(range(1880, 2025)):\n",
    "#         query = '''select ?film ?filmLabel ?title ?director ?directorLabel (year(?publication_date) as ?year) \n",
    "#             where {\n",
    "#                 ?film p:P31/wdt:P279* ?state .\n",
    "#                 ?state ps:P31/wdt:P279* wd:Q11424 .\n",
    "#                 ?film  wdt:P577 ?publication_date .\n",
    "#                 filter (year(?publication_date) = '''+str(year)+''') .\n",
    "#                 ?film wdt:P57 ?director\n",
    "#                 optional { ?film wdt:P1476 ?title } .\n",
    "#                 service wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}'''\n",
    "#         extract = sparql_query(query, \"https://query.wikidata.org/sparql\")\n",
    "#         wikidata = pandas.concat([wikidata, extract])\n",
    "\n",
    "#     for x in ['film', 'director']:\n",
    "#         wikidata[x] = wikidata[x].str.split('/').str[-1]\n",
    "\n",
    "#     wikidata = pandas.concat([\n",
    "#         wikidata[[x for x in wikidata.columns.values if x != 'filmLabel']],\n",
    "#         wikidata[[x for x in wikidata.columns.values if x != 'title']].rename(columns={'filmLabel':'title'})\n",
    "#         ]).dropna().drop_duplicates()\n",
    "\n",
    "#     wikidata = wikidata.rename(columns={\n",
    "#         'film':'film_wikidata', 'director':'director_wikidata', 'title':'film_label', 'directorLabel':'director_label'})\n",
    "    \n",
    "#     wikidata = wikidata.astype(str)\n",
    "#     wikidata.to_parquet(wikidata_path)\n",
    "# else:\n",
    "#     wikidata = pandas.read_parquet(wikidata_path)\n",
    "\n",
    "# wikidata['film_label'] = wikidata.apply(normalise, col='film_label', axis=1)\n",
    "# wikidata['director_label'] = wikidata.apply(normalise, col='director_label', axis=1)\n",
    "\n",
    "# print(len(wikidata)) \n",
    "# wikidata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_path = pathlib.Path.cwd() / 'match.parquet'\n",
    "\n",
    "# if not match_path.exists():\n",
    "\n",
    "#     name_match_score = 60 # name matching tolerance\n",
    "#     title_match_score = 100 # title matching tolerance\n",
    "#     minimum_match_candidates = 4 # minimum matching options.\n",
    "\n",
    "#     result_dataframe = pandas.DataFrame(columns=['Director', 'director_wikidata'])\n",
    "#     for x in tqdm.tqdm(dataframe.Director.unique()):\n",
    "#         focus = dataframe.loc[dataframe.Director.isin([x])]\n",
    "#         c = process.extract(x, wikidata.director_label.unique(), scorer=fuzz.WRatio, limit=200)\n",
    "#         c = [y[0] for y in c if y[1] > name_match_score]\n",
    "#         candidates = wikidata.loc[wikidata.director_label.isin(c)] \n",
    "#         result = {y:median_score(focus.Film.unique(), y, minimum_match_candidates) for y in candidates.director_wikidata.unique()}\n",
    "#         result = [k for k,v in result.items() if v == title_match_score]\n",
    "#         if len(result) == 1:\n",
    "#             result_dataframe.loc[len(result_dataframe)] = [(x), (result[0])]\n",
    " \n",
    "#     result_dataframe = result_dataframe.astype(str)\n",
    "#     result_dataframe.to_parquet(match_path)\n",
    "# else:\n",
    "#     result_dataframe = pandas.read_parquet(match_path)\n",
    "\n",
    "# print(len(result_dataframe))\n",
    "# result_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wd = wikidata.copy()\n",
    "# wd = wd[['director_wikidata', 'film_label', 'film_wikidata']]\n",
    "# wd = wd.rename(columns={'film_label':'Film'}).drop_duplicates()\n",
    "\n",
    "# # result = dataframe.copy()\n",
    "# dataframe = pandas.merge(dataframe, result_dataframe, on='Director', how='left')\n",
    "# dataframe = pandas.merge(dataframe, wd, on=['director_wikidata', 'Film'], how='left')\n",
    "\n",
    "# print(len(dataframe))\n",
    "# dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_dataframe = pandas.DataFrame()\n",
    "# for x in tqdm.tqdm(numpy.array_split(dataframe.dropna().film_wikidata.unique(), 10)):\n",
    "#     time.sleep(2)\n",
    "#     film_values = ' '.join([f'wd:{y}' for y in x])\n",
    "#     query = '''select ?film_wikidata ?wikidata_country \n",
    "#         where {\n",
    "#             values ?film_wikidata {'''+film_values+'''}\n",
    "#             ?film_wikidata wdt:P495 ?wikidata_country .}'''\n",
    "#     country_dataframe = pandas.concat([country_dataframe, sparql_query(query, \"https://query.wikidata.org/sparql\")])\n",
    "\n",
    "# for x in ['film_wikidata', 'wikidata_country']:\n",
    "#     country_dataframe[x] = country_dataframe[x].str.split('/').str[-1]\n",
    "\n",
    "# dataframe = pandas.merge(dataframe, country_dataframe, on='film_wikidata', how='left')\n",
    "# dataframe.loc[dataframe.country_wikidata == dataframe.wikidata_country, 'match'] = 'true'\n",
    "# dataframe.loc[~dataframe.match.isin(['true']), 'match'] = 'false'\n",
    "# dataframe = dataframe.sort_values(by='match', ascending=False)\n",
    "# dataframe = dataframe[['firstname', 'surname', 'film_wikidata', 'country',  'match']].dropna()\n",
    "# dataframe = dataframe.drop_duplicates(subset=['firstname', 'surname', 'film_wikidata', 'country'], keep='first')\n",
    "# dataframe = dataframe.loc[~dataframe.country.isin([''])]\n",
    "# dataframe['country'] = dataframe['country'].str.title()\n",
    "# dataframe['match'] = dataframe['match'].str.title()\n",
    "\n",
    "# print(len(dataframe)) \n",
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altair.data_transformers.enable('default', max_rows=None)\n",
    "# altair.Chart(dataframe).mark_bar().encode(\n",
    "#     x = altair.X('country', title='Country of voter.'),\n",
    "#     y=altair.Y('count(match)', stack=\"normalize\", title='Cinema of own country.'),\n",
    "#     color=altair.Color('match', scale=altair.Scale(domain=['True', 'False'], range=['#653E59', '#C9A6BE']),  \n",
    "#                        sort=['true', 'false'], title='')).properties(width=1500, height=300, title='Sight & Sound 2022 - Patriotic Voters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
