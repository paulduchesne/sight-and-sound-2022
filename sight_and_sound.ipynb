{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "import altair\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "import pathlib\n",
    "import pydash\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import unidecode\n",
    "\n",
    "def value_extract(row, col):\n",
    "\n",
    "    ''' Extract dictionary values. '''\n",
    "  \n",
    "    return pydash.get(row[col], \"value\")\n",
    "   \n",
    "def sparql_query(query, service):\n",
    " \n",
    "    ''' Send sparql request, and formulate results into a dataframe. '''\n",
    "\n",
    "    r = requests.get(service, params={\"format\": \"json\", \"query\": query})\n",
    "    data = pydash.get(r.json(), \"results.bindings\")\n",
    "    data = pandas.DataFrame.from_dict(data)\n",
    "    for x in data.columns:\n",
    "        data[x] = data.apply(value_extract, col=x, axis=1)\n",
    " \n",
    "    return data\n",
    "\n",
    "def normalise(row, col):\n",
    "\n",
    "    ''' Normalise text for matching purposes. '''\n",
    "\n",
    "    norm = unidecode.unidecode(str(row[col]).lower()).strip()\n",
    "\n",
    "    return norm\n",
    "\n",
    "def median_score(a_list, b_id, f):\n",
    "\n",
    "    ''' Find best match per against lists, return median. '''\n",
    "\n",
    "    test = wikidata.loc[wikidata.director_wikidata.isin([b_id])]\n",
    "    b_list = test.film_label.unique()\n",
    "    if len(a_list) < f or len(b_list) < f:\n",
    "        return 0\n",
    "\n",
    "    my_score = [process.extractOne(a, b_list, scorer=fuzz.WRatio)[1] for a in a_list]\n",
    "    return numpy.median(my_score)\n",
    "\n",
    "data_path = pathlib.Path.cwd() / 'sight_and_sound.parquet'\n",
    "\n",
    "if not data_path.exists():\n",
    "\n",
    "    index = requests.get('https://www.bfi.org.uk/sight-and-sound/greatest-films-all-time/all-voters').text\n",
    "    index = index.split('<script type=\"text/javascript\">var initialPageState = ')[1].split('</script>')[0]\n",
    "    dataframe = pandas.DataFrame(pydash.get(json.loads(index), 'componentState.allVoters'))\n",
    "    dataframe = dataframe[['firstname', 'surname', 'type', 'country', 'url']]\n",
    "\n",
    "    votes = pandas.DataFrame()\n",
    "    for x in tqdm.tqdm(dataframe.url.unique()):\n",
    "\n",
    "        time.sleep(4)\n",
    "\n",
    "        vote_page = pandas.read_html('https://www.bfi.org.uk'+x, encoding='utf8')[0]\n",
    "        vote_page['url'] = x\n",
    "        if len(vote_page) != 10:\n",
    "            print(pathlib.Path(x).stem, 'should be only ten votes')\n",
    "        votes = pandas.concat([votes, vote_page])\n",
    "\n",
    "    dataframe = pandas.merge(dataframe, votes, on='url', how='left')\n",
    "    dataframe = dataframe.astype(str)\n",
    "    dataframe.to_parquet(data_path)\n",
    "else:\n",
    "    dataframe = pandas.read_parquet(data_path)\n",
    "\n",
    "dataframe['Film'] = dataframe.apply(normalise, col='Film', axis=1)\n",
    "dataframe['Director'] = dataframe.apply(normalise, col='Director', axis=1)\n",
    "\n",
    "print(len(dataframe))\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['country'] = dataframe['country'].str.split('/')\n",
    "dataframe = dataframe.explode('country')\n",
    "dataframe['country'] = dataframe.apply(normalise, col='country', axis=1)\n",
    "\n",
    "dataframe = dataframe.replace({'country': {\n",
    "    'uk':'united kingdom', 'england':'united kingdom', 'usa':'united states of america', \n",
    "    'united states':'united states of america', 'us':'united states of america', \n",
    "    'china':\"people's republic of china\", 'ireland':'republic of ireland',\n",
    "    'canda':'canada', 'palestine':'state of palestine', 'finnland':'finland',\n",
    "    'macedonia':'north macedonia', 'abu dhabi':'united arab emirates'}})\n",
    "\n",
    "query = '''select ?country ?countryLabel \n",
    "    where {\n",
    "      values ?status {wd:Q3624078 wd:Q6256 wd:Q779415}\n",
    "        ?country wdt:P31 ?status .\n",
    "        service wikibase:label { bd:serviceParam wikibase:language \"en\". }}'''\n",
    "\n",
    "countries = sparql_query(query, \"https://query.wikidata.org/sparql\")\n",
    "countries = countries.rename(columns={'country':'country_wikidata'})\n",
    "countries = countries.rename(columns={'countryLabel':'country'})\n",
    "countries['country'] = countries.apply(normalise, col='country', axis=1)\n",
    "countries['country_wikidata'] = countries['country_wikidata'].str.split('/').str[-1]\n",
    "\n",
    "dataframe = pandas.merge(dataframe, countries, on='country', how='left').drop_duplicates()\n",
    "\n",
    "print(len(dataframe))\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_path = pathlib.Path.cwd() / 'wikidata.parquet'\n",
    "\n",
    "if not wikidata_path.exists():\n",
    "    wikidata = pandas.DataFrame()\n",
    "    for year in tqdm.tqdm(range(1880, 2025)):\n",
    "        query = '''select ?film ?filmLabel ?title ?director ?directorLabel (year(?publication_date) as ?year) \n",
    "            where {\n",
    "                ?film p:P31/wdt:P279* ?state .\n",
    "                ?state ps:P31/wdt:P279* wd:Q11424 .\n",
    "                ?film  wdt:P577 ?publication_date .\n",
    "                filter (year(?publication_date) = '''+str(year)+''') .\n",
    "                ?film wdt:P57 ?director\n",
    "                optional { ?film wdt:P1476 ?title } .\n",
    "                service wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}'''\n",
    "        extract = sparql_query(query, \"https://query.wikidata.org/sparql\")\n",
    "        wikidata = pandas.concat([wikidata, extract])\n",
    "\n",
    "    for x in ['film', 'director']:\n",
    "        wikidata[x] = wikidata[x].str.split('/').str[-1]\n",
    "\n",
    "    wikidata = pandas.concat([\n",
    "        wikidata[[x for x in wikidata.columns.values if x != 'filmLabel']],\n",
    "        wikidata[[x for x in wikidata.columns.values if x != 'title']].rename(columns={'filmLabel':'title'})\n",
    "        ]).dropna().drop_duplicates()\n",
    "\n",
    "    wikidata = wikidata.rename(columns={\n",
    "        'film':'film_wikidata', 'director':'director_wikidata', 'title':'film_label', 'directorLabel':'director_label'})\n",
    "    \n",
    "    wikidata = wikidata.astype(str)\n",
    "    wikidata.to_parquet(wikidata_path)\n",
    "else:\n",
    "    wikidata = pandas.read_parquet(wikidata_path)\n",
    "\n",
    "wikidata['film_label'] = wikidata.apply(normalise, col='film_label', axis=1)\n",
    "wikidata['director_label'] = wikidata.apply(normalise, col='director_label', axis=1)\n",
    "\n",
    "print(len(wikidata)) \n",
    "wikidata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_path = pathlib.Path.cwd() / 'match.parquet'\n",
    "\n",
    "if not match_path.exists():\n",
    "\n",
    "    name_match_score = 60 # name matching tolerance\n",
    "    title_match_score = 100 # title matching tolerance\n",
    "    minimum_match_candidates = 4 # minimum matching options.\n",
    "\n",
    "    result_dataframe = pandas.DataFrame(columns=['Director', 'director_wikidata'])\n",
    "    for x in tqdm.tqdm(dataframe.Director.unique()):\n",
    "        focus = dataframe.loc[dataframe.Director.isin([x])]\n",
    "        c = process.extract(x, wikidata.director_label.unique(), scorer=fuzz.WRatio, limit=200)\n",
    "        c = [y[0] for y in c if y[1] > name_match_score]\n",
    "        candidates = wikidata.loc[wikidata.director_label.isin(c)] \n",
    "        result = {y:median_score(focus.Film.unique(), y, minimum_match_candidates) for y in candidates.director_wikidata.unique()}\n",
    "        result = [k for k,v in result.items() if v == title_match_score]\n",
    "        if len(result) == 1:\n",
    "            result_dataframe.loc[len(result_dataframe)] = [(x), (result[0])]\n",
    " \n",
    "    result_dataframe = result_dataframe.astype(str)\n",
    "    result_dataframe.to_parquet(match_path)\n",
    "else:\n",
    "    result_dataframe = pandas.read_parquet(match_path)\n",
    "\n",
    "print(len(result_dataframe))\n",
    "result_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = wikidata.copy()\n",
    "wd = wd[['director_wikidata', 'film_label', 'film_wikidata']]\n",
    "wd = wd.rename(columns={'film_label':'Film'}).drop_duplicates()\n",
    "\n",
    "# result = dataframe.copy()\n",
    "dataframe = pandas.merge(dataframe, result_dataframe, on='Director', how='left')\n",
    "dataframe = pandas.merge(dataframe, wd, on=['director_wikidata', 'Film'], how='left')\n",
    "\n",
    "print(len(dataframe))\n",
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dataframe = pandas.DataFrame()\n",
    "for x in tqdm.tqdm(numpy.array_split(dataframe.dropna().film_wikidata.unique(), 10)):\n",
    "    time.sleep(2)\n",
    "    film_values = ' '.join([f'wd:{y}' for y in x])\n",
    "    query = '''select ?film_wikidata ?wikidata_country \n",
    "        where {\n",
    "            values ?film_wikidata {'''+film_values+'''}\n",
    "            ?film_wikidata wdt:P495 ?wikidata_country .}'''\n",
    "    country_dataframe = pandas.concat([country_dataframe, sparql_query(query, \"https://query.wikidata.org/sparql\")])\n",
    "\n",
    "for x in ['film_wikidata', 'wikidata_country']:\n",
    "    country_dataframe[x] = country_dataframe[x].str.split('/').str[-1]\n",
    "\n",
    "dataframe = pandas.merge(dataframe, country_dataframe, on='film_wikidata', how='left')\n",
    "dataframe.loc[dataframe.country_wikidata == dataframe.wikidata_country, 'match'] = 'true'\n",
    "dataframe.loc[~dataframe.match.isin(['true']), 'match'] = 'false'\n",
    "dataframe = dataframe.sort_values(by='match', ascending=False)\n",
    "dataframe = dataframe[['firstname', 'surname', 'film_wikidata', 'country',  'match']].dropna()\n",
    "dataframe = dataframe.drop_duplicates(subset=['firstname', 'surname', 'film_wikidata', 'country'], keep='first')\n",
    "dataframe = dataframe.loc[~dataframe.country.isin([''])]\n",
    "dataframe['country'] = dataframe['country'].str.title()\n",
    "dataframe['match'] = dataframe['match'].str.title()\n",
    "\n",
    "print(len(dataframe)) \n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altair.data_transformers.enable('default', max_rows=None)\n",
    "altair.Chart(dataframe).mark_bar().encode(\n",
    "    x = altair.X('country', title='Country of voter.'),\n",
    "    y=altair.Y('count(match)', stack=\"normalize\", title='Cinema of own country.'),\n",
    "    color=altair.Color('match', scale=altair.Scale(domain=['True', 'False'], range=['#653E59', '#C9A6BE']),  \n",
    "                       sort=['true', 'false'], title='')).properties(width=1500, height=300, title='Sight & Sound 2022 - Patriotic Voters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
